# MT Evaluation Consistency Study (WMT 2021â€“2023)

This repository contains experiments investigating the **consistency of machine translation evaluation metrics** across different **domains** and **language pairs** using data from the WMT 2021, 2022, and 2023 Metrics Shared Tasks.

---

## ğŸŒ Project Goals

- Assess whether leading **MT evaluation metrics** (e.g., BLEU, COMET, BERTScore, MetricX-24) perform consistently across:
  - Different **language pairs**
  - Different **domains** (news, , , etc.)
- Compare metric performance with **human judgment** (DA/MQM correlation)
- Evaluate the **robustness and generalizability** of metrics over multiple years

---

## ğŸ“Š Selected Metrics

Metrics currently under evaluation:

- [x] COMET-22
- [x] COMET-KIWI
- [x] BLEURT-20
- [x] BERTScore
- [ ] xCOMET
- [ ] MetricX-24
- [ ] GPT-4 evaluator (if available)

See [`metrics/`](metrics/) for details on each.

---

## ğŸ“ Repository Structure

```bash
MT_EvalMetri_Lab/
â”œâ”€â”€ data/               # Raw and preprocessed WMT data
â”‚   â”œâ”€â”€ wmt21/
â”‚   â”œâ”€â”€ wmt22/
â”‚   â””â”€â”€ wmt23/
â”œâ”€â”€ metrics/            # Scripts or wrappers for each metric
â”œâ”€â”€ scripts/            # Data preprocessing and scoring scripts
â”œâ”€â”€ notebooks/          # Jupyter notebooks for analysis and visualization
â”œâ”€â”€ results/            # Output metric scores and comparison results
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md
